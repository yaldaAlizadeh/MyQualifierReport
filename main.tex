%\textsc{\char39} is apasteroph
% This is "sig-alternate.tex" V2.0 May 2012
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% * <yalda.fazlalizadeh62@gmail.com> 2017-12-01T06:52:51.347Z:
%
% ^.
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and 
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate}
\usepackage{comment}
\usepackage{multirow}
\usepackage{bbding}
\usepackage{wrapfig}
\usepackage{placeins}
\usepackage{graphicx}
\usepackage{mathtools}


\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{SOSP}{'97 El Paso, Texas USA}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---
%previous title: Test Case Prioritization Based on Software Quality Metrics Incorporating Code Paradigm Data
\title{Decentralized Task Scheduling Approach for Running Many-Task Application in the Cloud\titlenote{(Produces the permission block, and
copyright information). For use with
SIG-ALTERNATE.CLS. Supported by ACM.}}
%\subtitle{[Extended Abstract]
%\titlenote{A full version of this paper is available as
%\textit{Author's Guide to Preparing ACM SIG Proceedings Using
%\LaTeX$2_\epsilon$\ and BibTeX} at
%\texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{2} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Yalda Fazlalizadeh\titlenote{Computer Science PhD student.}\\
	   \affaddr{Louisiana State University}\\
       \affaddr{Baton Rouge, LA, USA}\\
       \email{\textnormal{yfazla1@lsu.edu}}
% 2nd. author
\alignauthor
Gerald Baumgartner\titlenote{Associate Professor in Computer Science Department, Louisiana State University.}\\
       \affaddr{Louisiana State University}\\
       \affaddr{Baton Rouge, LA, USA}\\
      \email{\textnormal{gb@csc.lsu.edu}}
}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
%\additionalauthors{Additional authors: John Smith (The Th{\o}rv{\"a}ld Group,
%email: {\texttt{jsmith@affiliation.org}}) and Julius P.~Kumquat
%(The Kumquat Consortium, email: {\texttt{jpkumquat@consortium.net}}).}
\date{20 March 2018}
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
 
\end{abstract}

\begin{comment}

\end{comment}

\section{Introduction}
Today, the enormous computation power and resource costs required in many scientific fields, such as astronomy, quantum chemistry, geophysics, or bioinformatics, makes it not feasible anymore to use the historical patterns for using the power of combined resources such as grids of desktop-based master-worker computers or traditional supercomputer grids. In the past decade, these problems have been addressed using cloud~computing and underlying technologies as an alternative to perform computations in distributed environments. Cloud~computing is useful to run wide range of applications such as generic word processing softwares to customized enterprise computer programs. It has changed the face of data storage and access through the numerous innovations to the field. Using this technology, today, the required hardware costs for mass computations has brought down to a large degree, while the information processing and computation speed has increased. However, the variations of resources in the cloud environment makes it difficult for many scientific applications that involve complicated communication and collaboration between sub-tasks, such as large dense matrix multiplications, to efficiently run on the cloud. The reason for this is because of performance variation of underlying resources in cloud~system, which makes the overall performance of computation obtain form different nodes in the overlay network to be unpredictable, and in many cases inefficient.

The reason for cloud's inefficiency for running large high-performance computing applications is because of the underlying virtualization technology that made cloud~system's implementation possible. With the increasing number of applications hosted by cloud providers, that mostly face high levels of fluctuations in computation intensity the allocated virtual machines will dynamically power up and down. Therefore, the providers have to examine different methods to optimize the energy consumption while keeping the computation transparency. This often leads to providers allocating fewer physical resources and therefore, virtual~machines' co-location on same devices and competing over CPU clock-time and other available resources and potential performance degradations~\cite{papadopoulos:virtual}. On the other side, code (task) migration within the distributed network of nodes in the cloud also brings concerns about internal network contention and saturation. Under such connections, the competitions over bandwidth, such as data ingest, $\textnormal{customer'}$ inner services communication, required IaaS communications, and possibly communications due to the proposed VM migrations, will affect the bandwidth~usage~\cite{raycroft:Performance}. This is a major hindrance toward cloud scalability and mandates attempting real-world policies and realistic testing scenarios to determine the optimal setup to solve the problem of volatile network bandwidth for a specific cloud.

For applications that can be broken into a set of smaller tasks, however, it may be possible to match the performance requirements of a task with the performance characteristics of (a subset of) the cloud nodes.  For grids and supercomputers, this {\em many-task computing\/} approach \cite{raicu:many} has proven very effective.  E.g., Raj\-bhandari et al.~\cite{Sriram_SC14b} structured the computation of a quantum chemistry tensor contraction equation as a task graph with dependencies and were able to scale the computation to over 250,000 cores.  They dynamically schedule tasks on groups of processors and
use work-stealing for load balancing.

One solution to run such many-task applications in the cloud environment is to identify and keep the updated record of the performance characteristics of the nodes and network condition between the nodes, centrally. However, maintaining this information for a large number of machines is prohibitively expensive. IBM's Air Terrific Controller (ATC) algorithm~\cite{barsness:2014:Distributed} attempts to solve this problem through having a group of controller nodes that maintain the performance information of a group of the available worker nodes, The controller nodes also compete over the available tasks in the central task-queue with other controller nodes, to fetch the one which is the best fit to the computing power of the subgroup of worker nodes. While this approach distributes the load of maintaining performance information and also makes the sub-optimal task assignment to the cloud nodes, the central job queue is still a potential source of bottleneck. In addition, the central controller nodes may not have enough information about the computational requirements and communication patterns of the individual tasks.

In this research, we aim at using a fully-decentralized approach in which, the applications decide on which nodes to run. This approach relies on the nodes advertising the performance information periodically. This information is available either through the hypervisor node or operating system. Otherwise, the nodes measure it in certain time frequencies and distribute them to neighboring nodes. The computational tasks can be added to the cloud system at anytime resembling actual many-task application systems. When making a decision about whether perform a job locally, or move it through the overlay network, the application needs to query the required resources' availability in the overlay network, and equilibrate this information with those of job movement burden, to find the best spot to complete the job. This is a privilege of this approach for applications, which might not be available if the decision is made by leader nodes. 

In order to decide about the direction of task movement in the cloud network, we plan to use a vector-based model. In this approach, the performance characteristics of each node is described as a vector of numerical coefficients. Plotting the nodes on an N-dimensional space, the coefficient vector and the dot-product of this vector to those available from other nodes, it is possible to find a node with highest similarity to the performance characteristics to the initiator node's vector. Therefore, it is possible to think about the vector as the direction to which the work should move. To make the vector-based decision about work migration, the main criteria is to move the jobs from longer job queues to the (nodes' with) shorter queues. However, the demonstrated computation performance of the nodes is a decidedly important factor to this. So, one important step in this research is to find the optimized vector to minimize the computation time for jobs distributing through the cloud of nodes.

The main research questions we aim to answer in this research are as follows: 
1 - ....

The rest of this report has organized as follows: Section 2 summarizes the related work and introduces our motivations for the research. Section 3 explains the design of vector-based job scheduling framework and how we fit that for objectives of the research. Section 4 describes the speculated plans to improve the scheduling of the tasks to obtain better overall performance results in mass-computation compared to the previous work~\cite{peterson:decentralized} in this area. Section 5 briefly describes tensor contraction computation, as a potential application to apply our decentralized scheduling approach in. Section 6 concludes the report.

\section{Problem Statement and Motivation}
\subsection{Problem of Current Scheduling Algorithms}
Prior work has demonstrated the possibility of combining the two centralized and decentralized strategies for work scheduling. The biologically inspired, Organic Grid~\cite{chakravarti:organic} approach attempts to maximize the resource utilization through fully-decentralized task-scheduling. Since this approach has developed to address the current limitations of cloud applications to utilize the planetary scale computation power of desktop grids and networks cannot be assumed to be reliable, the Organic Grid scheduling approach is able to adapt to the cloud systems with unreliable network quality. One heuristic method that used in this approach is to hierarchically move high performing nodes closer to the source of work. However, it assumes only one single source of work and builds the overlay network centered around it, while in large-scale cloud systems is not necessarily hold. An even more serious challenge with this approach is its high overhead mostly related to destruction and recreation of the overlay network around the point to which a new work added. Moreover, the agents, which encapsulate the behavior and computation, are responsible to organize the overlay network in the Organic Grid. However, since the agents are associated with individual jobs, destruction of the network and agents, as a result, leads to missing the available knowledge in the previous session is not available for the next session. One can see that, although Organic Grid had notable achievements scheduling large amounts of tasks from a consistent insertion point, yet, there is a room for improvement.

The ATC algorithm, on the other hand, attempts to perform a mid-level centralization, such that, applications make the decision to perform the jobs on the nodes based on the information they obtain about the availability of required resources' for the job for underlying nodes. Although this level of centralization and workers competing over tasks to get the best fitting job based on the controller's knowledge about the available group of workers has experimentally proven to improve the performance, the communication burden of controller nodes in this approach is a non-negligible surplus. Moreover, as mentioned before, the centralized job queue is a potential point of failure in this approach as there is a possibility of bottleneck formation as a result of controller nodes' unsystematic accessing it. Also, the air traffic controllers may not have enough information about the computational requirements and communication pattern of individual tasks.

In prior work\cite{peterson:decentralized}, the proposed approach is fully decentralized, and the applications decide on which nodes to run, which can improve the performance of some applications in the cloud. The approach relies on the nodes to advertise their performance characteristics. This information will be available to use from hypervisor or operating system; otherwise, the nodes need to periodically measure their own performance related metrics. Then, they distribute this information into the adjacent neighboring nodes. Similar to realistic applications in cloud environments, computational tasks can add up at any point. The applications check the availability of the resources through queries in an API, in order to move the tasks to the node with appropriate availability of required resources. With applications being able to decide about the node to run the work on, they can best distribute the tasks having the information about the communication and computation characteristics of the application that would not be available if the decision is made by leader nodes.

In prior work~\cite{peterson:decentralized}, the authors measure the performance of the nodes as one important criteria to make the decision about job movement on its basis. However, the problem is that the nodes are prioritized based on their history of faster executing identical jobs. In the practical cloud environments however, there is every possibility that computation tasks with different sizes are added to the cloud. In this research, we are attempting to address this issue through considering the scheduling of the tasks of variable sizes to the nodes. As to the prior work~\cite{peterson:decentralized} the criteria to move the task around the network is from longer queue lists to shorter ones, and also, to the high-performing nodes. To measure the performance of the nodes, one solution would be to use \textit{Performance Software Probes}~\cite{strube:probe}, that follows a standard, such as Flick\cite{salah:performance}, to periodically run on each node. During node's advertising its performance relevant characteristics, the measurements done by the probe software is also included as an index to the node's performance in completing jobs. These standard probes help to measure the performance of the executing nodes more precisely. In the meantime, the probe software also use CPU clocks, and we need to find a way such that this time burden does not overshadow the benefits of using probes.

Another problem with the prior research\cite{peterson:decentralized} is that to have nodes of different computing power and variable computation performances in the cloud, they entered some dummy computations in the code in order to artificially slow down the computation speed of the nodes. However, to make it more realistic, one possibility is to use realistic background applications that run on the same machine through a parallel VM installed on top of the hypervisor. An example for such computation-intensive applications would be search queries, such as Google searches, of different degrees of complexity.

Another point of improvement is to use actual variable computing resources from a hybrid cloud environment rather than using the homogeneous cloud provided by the CloudLab during the experiments of prior work\cite{peterson:decentralized}. This helps to get more practical results than we can get with machines with relatively identical configurations and hardware facilities.

One possibility in the actual cloud environments is having a sequence of tasks need to be completed in an specific order. In other words, to have valid computation results, we need to consider a task graph needs to be followed precisely. In the prior work\cite{peterson:decentralized} there is no evidence found regarding that they have considered the order of completion of the tasks in the series of conducted experiments. We are attempting to consider task dependency as one additional factor in calculations to distribute the tasks to the network of nodes in the cloud.

The other respect could be improved with the prior work\cite{peterson:decentralized} would be repeating the experiments having tasks that need greater computation power than what is obtainable from the node on which the task is running. Considering this, we need to address three relevant issues arising from this: First, considering the required communication and collaboration between the nodes on which the task is running, we need to find a way for applications to query the candidate sub-group of nodes for running the task to find the group with the most efficient inter-communication possibility. The second thing to consider for the application to make a maximally optimized decision and determine the direction of moving the work in the overlay network is to find a sub-group with the best combined performance characteristics to the requirements of the task. Last but not least, there need to be a possibility to prevent the applications from assigning the tasks to the first best fitting candidate sub-group of nodes regardless of the future computation tasks entering the system. In other words, the current decision to assign the task to the sub-groups of nodes must be made also aware of resource utilization optimizations, which is an NP-complete problem and requires different heuristic approaches to be examined\cite{sujan:efficient,mahendra:task-scheduling}.   



\subsection{Motivation for Vector-Based Scheduling}
In decentralized task scheduling, every decision must be dine based on the limited amount of knowledge at a certain point in the network. As to the prior work~\cite{peterson:decentralized}, our overlay network is also composed of groups of worker nodes that organize and assigned with tasks through the controller nodes. However, the worker nodes have this option to decide whether to complete the job locally or move it to an adjacent node.

\subsection{Testing Benchmark}
The experimental measurements in the previous work\cite{peterson:decentralized} demonstrated that a meaningful correlation between the measured performance metrics of the nodes and their history of speed-to-complete-the-job does not necessarily hold. In order to achieve a dependable level in the accuracy of the results, their reproducibility and comparability with those of the previous works in this area~\cite{peterson:decentralized, Evangelinos08cloudcomputing} we use a subset of the NAS Benchmark set, which is a commonly used benchmark to test cloud computing platforms. The NAS Parallel Benchmarks (NPB) are a small set of programs designed to help evaluate the performance of parallel supercomputers~\cite{nasbenchmark:2016}.

\subsection{An Application for Proposed Approach}

In spite of striking advancements in the area of electronics, computational engines and communication algorithms, the development of high-performance parallel programs in many scientific areas still remains as a complicated challenge~\cite{baumgartner:compiler}. The complexity of such problems is due to the many factors that their effects needs to be considered. The choice of the algorithms to access the memory, costs, and communication overheads are only a few examples of these factors.
Moreover, with the common resource and time limitations while developing software projects,  the high-performance optimization, efficient parallelism capabilities for computational models, and in general, the quality of the final products are inevitably compromised. Therefore, the approaches that automatically produce the high-performance parallel programs in scientific applications are highly demanding.
One of such applications is to automatically develop program synthesis tools to facilitate the development of high-performance parallel programs for a class of scientific computations encountered in chemistry
and physics. The required calculations in the area involve large collections of tensor contractions (generalized matrix multiplications). Currently, chemists spent weeks or months manipulating formulas containing dozens or hundreds of terms with Mathematica, hand-optimizing the computation, and writing low-level code by hand~\cite{tce:tensor:2015}. This computationally intensive calculations to express set of tensor contractions can typically take on the order of 1 TFLOP week or more and can require multiple TBs of storage.


In particular, the computations comprise the bulk
of the computation with the coupled cluster approach to the accurate description of the electronic structure of atoms and molecules~\cite{martin:encyclopedia,lee:achieving}. Computational approaches to modeling the structure and interactions of molecules, the electronic and optical properties of molecules, the heats and rates of chemical reactions, etc., are crucial to the understanding of chemical processes in real-world systems. Examples of applications include combustion and atmospheric chemistry, chemical vapor deposition, protein structure and enzymatic chemistry, and industrial chemical processing. A proper computation facility to efficiently perform the many-task application of tensor contraction computation is usually assumed to be powerful super-computers. However, Computational chemistry and materials science account for significant fractions of supercomputer usage at national centers (for example, approximately 85\% of total usage at Pacific Northwest National Laboratories, 30\% at NERSC, and about 50\% of SDSC's IBM SP/128 system). With the growing size of already gigantic computations and needs fast and efficient scalability of the underlying network of computers, and considering the many benefits that the cloud computing systems in mass-computations have, we motivated to focus on implementing this application to perform on high-performance cloud adapted for many-task applications with the decentralized scheduling of the tasks as described.


Precise calculation of molecular electronic wavefunctions by methods such as coupled-cluster requires
the computation of tensor contractions, the cost of which has polynomial computational scaling
with respect to the system and basis set sizes. Each contraction may be executed via matrix multiplication
on a properly ordered and structured tensor. However, data transpositions are often needed to
reorder the tensors for each contraction. Writing and optimizing distributed-memory kernels for each
transposition and contraction is tedious since the number of contractions scales combinatorially with the
number of tensor indices.


This paper provides an overview of a project that is developing
a program synthesis system to facilitate the rapid
development of high-performance parallel programs for a
class of scientific computations encountered in chemistry
and physics—electronic structure calculations, where many
computationally intensive components are expressible as a
set of tensor contractions. Currently, manual development
of accurate quantum chemistry models in this domain is
very tedious and takes an expert several months to years
to develop and debug. The synthesis tool aims to reduce
the development time to hours/days, by having the chemist
specify the computation in a high-level form, from which an
efficient parallel program is automatically synthesized. This
should enable the rapid synthesis of high-performance implementations
of sophisticated ab initio quantum chemistry
models, including models that are too tedious for manual
development by quantum chemists. Fig. 1 shows a tensor
contraction expression for one of the terms in the coupled
cluster model [43], [47] for ab initio electronic structure
modeling. An optimized parallel Message Passing Interface
(MPI) program to implement such an expression containing
a large number of tensor products typically requires several
thousands of lines of code.


This paper provides an overview of a program synthesis system
for a class of quantum chemistry computations. These computations
are expressible as a set of tensor contractions and arise in electronic
structure modeling. The input to the system is a a high-level specification
of the computation, from which the system can synthesize
high-performance parallel code tailored to the characteristics of the
target architecture. Several components of the synthesis system are described, focusing on performance optimization issues that they
address.




%needs to be modified
\section{Background and Related Work}



\begin{comment}
\begin{table}
\centering
\caption{Frequency of Special Characters}
\begin{tabular}{|c|c|l|} \hline
Non-English or Math&Frequency&Comments\\ \hline
\O & 1 in 1,000& For Swedish names\\ \hline
$\pi$ & 1 in 5e& Common in math\\ \hline
\$ & 4 in 5 & Used in business\\ \hline
$\Psi^2_1$ & 1 in 40,000& Unexplained usage\\
\hline\end{tabular}
\end{table}
\end{comment}

\begin{comment}
\begin{table*}
\centering
\caption{Experiment Overview.}
\begin{tabular}{|c l|} \hline
\textbf{Goal} & \multicolumn{1}{p{13cm}|}{Study the effect of re-ordering and inclusion/exclusion of Software quality metrics as two treatments in fault detection effectiveness of prioritization techniques.} \\ \hline
\textbf{Main
Factor}& Scheduling of test cases for Software quality metric based prioritization technique.\\ \hline
\textbf{Dependent
variables}& Effectiveness in terms of fast detection of faults.\\ \hline
\textbf{Independent variables}& Software quality metrics, Program structure type, Granularity of code coverage.\\ \hline\end{tabular}
\end{table*}

\begin{table*}
\centering
\caption{The subject programs used in the experiments \cite{henard:comparing}.}
\resizebox{\textwidth}{!}{
%\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{|l c l r r r r r r |}   
  \hline
\textbf{Program} & \textbf{Test Cases} &  \textbf{Information} & \textbf{V0} &
\textbf{V1} & \textbf{V2} & \textbf{V3} & \textbf{V4} & \textbf{V5} \\ 
\hline   %\hline
 & & Version & 2.2(1996) & 2.2 (1998) & 2.3 (1999) & 2.4 (1999) & 2.5 (2002) & 2.7 (2010) \\
 GREP & 440 & Size & 8,163 & 11,988 & 12,724 & 12,826 & 20,838 & 58,344 \\
 & & Faults & - & 56 & 58 & 54 & 59 & 59 \\
 
 & & Version & 3.01 (1998) & 3.02 (1998) & 4.0.6 (2003)  & 4.0.8 (2003)  & 4.1.1 (2004) & 4.2 (2009) \\
 SED & 324 & Size & 7,790 & 7,793 & 18,545 & 18,687 & 21,743 & 26,466 \\
 & & Faults & - & 16 & 18 & 18 & 19 & 22 \\
 
  & & Version & 2.4.3 (1993) & 2.4.7 (1994) & 2.5.1 (1995) & 2.5.2 (1996)  & 2.5.3 (1996) 2.5.4 & (1997) (2004)\\
 FLEX & 500 & Size & 8,959 & 9,470 & 12,231 & 12,249 & 12,379 & 12,366 \\
 & & Faults & - & 32 & 32 & 20 & 33 & 32 \\
 
   & & Version & 3.75 (1996) & 3.76.1 (1997)  & 3.77 (1998)  & 3.78.1 (1999) & 3.79 (2000) & 3.80 (2002)\\
 MAKE & 111 & Size & 17,463 & 18,568 & 19,663 & 20,461 & 23,125 & 23,400 \\
 & & Faults & - & 37 & 29 & 28 & 29 & 28 \\
 
    & & Version & 1.0.7 (1993)  & 1.1.2 (1993) & 1.2.2 (1993) & 1.2.3 (1993) & 1.2.4 (1993) & 1.3 (1999)\\
 GZIP & 156 & Size & 4,324 & 4,521  & 5,048 & 5,059 & 5,178 & 5,682 \\
 & & Faults & - & 8 & 8 & 7 & 7 & 7 \\ \hline
 
\end{tabular}
%\end{adjustbox}
}
\end{table*}
\end{comment}

% end the environment with {table*}, NOTE not {table}!
\begin{comment}
\subsection{Figures}

\begin{figure}
\centering
\epsfig{file=fly.eps}
\caption{A sample black and white graphic (.eps format).}
\end{figure}

\begin{figure}
\centering
\epsfig{file=Understand.eps, width=3.3in}
\caption{Understand\texttrademark tool IDE environment.}
\end{figure}

\begin{figure*}
\centering
\resizebox{\textwidth}{!}{
\epsfig{file=studyOneBoxPlots.eps}
}
\caption{APFD values for the state-of-the-art (SA) versus proposed approach (PR) on V1 to V5.}

\end{figure*}
\end{comment}


%Our conducted experiment addresses a real problem, i.e. the differences in individual performance and the understanding of the differences.


\subsection{Experimental Goal and Hypotheses}

\begin{comment}
We can reject the null
hypothesis $H0 _{PST}$ considering all tasks in both Groups A
and B (p-value=0.000036 and 0.00005 respectively).
The effect size is medium (0.51 and 0.55) for both
groups.
\end{comment}



\begin{comment}
We report on the effect of layout on accuracy and speed in all task categories as
well as in each of the six task categories. Effect size is
also reported using Cohen's d for accuracy and speed to
facilitate easy comparison to other studies. 
\end{comment}


%\begin{itemize}
%\item granularity of or code coverage info.
%\item ML algorithm
%\item programs used(context and nature cause some effect)
%\item  Seeded faults
%\item  random idea to break ties.
%\end{itemize}

\begin{comment}
\begin{itemize}
\item  implementation of other SQM based TCP technique.
\item code coverage extraction tool.
\item Faulty versions

There are several validity threats to the design of this study.
Our choice of..
should of course..
this would allow.....would give...

During data collection we mostly used a...

Another threat to the data collection is that our chosen...

did not always fit..

Since we had no...
\end{comment}


\section{Conclusions}
In this study we ..


%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{main}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns

\end{document}
